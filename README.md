## ğŸ“– Introduction

**Medical_Qwen** è®­ç»ƒåŒ»ç–—å¤§æ¨¡å‹ï¼Œå®ç°äº†åŒ…æ‹¬å¢é‡é¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒã€RLHF(å¥–åŠ±å»ºæ¨¡ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒ)å’ŒDPO(ç›´æ¥åå¥½ä¼˜åŒ–)ã€GRPOç­‰,è¿˜æœ‰å¯¹åº”çš„æµ‹è¯•ä»£ç ï¼ŒåŒ…æ‹¬BLEU1-4ã€ROUGE-Lç­‰æŒ‡æ ‡çš„è®¡ç®—ã€‚

TODO: å°è¯•æ”¹è¿›æ¨¡å‹çš„æ¡†æ¶ï¼Œåœ¨å·²æœ‰æ¨¡å‹ä¸Šå¢åŠ RAGå’Œè¾“å…¥æµçš„å› æœæ¨æ–­
<img src="https://github.com/shibing624/MedicalGPT/blob/main/docs/dpo.jpg" width="860" />


## ğŸ˜Š Features


åŸºäºChatGPT Training Pipelineï¼Œæœ¬é¡¹ç›®å®ç°äº†é¢†åŸŸæ¨¡å‹--åŒ»ç–—è¡Œä¸šè¯­è¨€å¤§æ¨¡å‹çš„è®­ç»ƒï¼š


- ç¬¬ä¸€é˜¶æ®µï¼šPT(Continue PreTraining)å¢é‡é¢„è®­ç»ƒï¼Œåœ¨æµ·é‡é¢†åŸŸæ–‡æ¡£æ•°æ®ä¸ŠäºŒæ¬¡é¢„è®­ç»ƒGPTæ¨¡å‹ï¼Œä»¥é€‚åº”é¢†åŸŸæ•°æ®åˆ†å¸ƒï¼ˆå¯é€‰ï¼‰
- ç¬¬äºŒé˜¶æ®µï¼šSFT(Supervised Fine-tuning)æœ‰ç›‘ç£å¾®è°ƒï¼Œæ„é€ æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œåœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸ŠåšæŒ‡ä»¤ç²¾è°ƒï¼Œä»¥å¯¹é½æŒ‡ä»¤æ„å›¾ï¼Œå¹¶æ³¨å…¥é¢†åŸŸçŸ¥è¯†
- ç¬¬ä¸‰é˜¶æ®µ
  - RLHF(Reinforcement Learning from Human Feedback)åŸºäºäººç±»åé¦ˆå¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œåˆ†ä¸ºä¸¤æ­¥ï¼š
    - RM(Reward Model)å¥–åŠ±æ¨¡å‹å»ºæ¨¡ï¼Œæ„é€ äººç±»åå¥½æ’åºæ•°æ®é›†ï¼Œè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œç”¨æ¥å»ºæ¨¡äººç±»åå¥½ï¼Œä¸»è¦æ˜¯"HHH"åŸåˆ™ï¼Œå…·ä½“æ˜¯"helpful, honest, harmless"
    - RL(Reinforcement Learning)å¼ºåŒ–å­¦ä¹ ï¼Œç”¨å¥–åŠ±æ¨¡å‹æ¥è®­ç»ƒSFTæ¨¡å‹ï¼Œç”Ÿæˆæ¨¡å‹ä½¿ç”¨å¥–åŠ±æˆ–æƒ©ç½šæ¥æ›´æ–°å…¶ç­–ç•¥ï¼Œä»¥ä¾¿ç”Ÿæˆæ›´é«˜è´¨é‡ã€æ›´ç¬¦åˆäººç±»åå¥½çš„æ–‡æœ¬
  - [DPO(Direct Preference Optimization)](https://arxiv.org/pdf/2305.18290.pdf)ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•ï¼ŒDPOé€šè¿‡ç›´æ¥ä¼˜åŒ–è¯­è¨€æ¨¡å‹æ¥å®ç°å¯¹å…¶è¡Œä¸ºçš„ç²¾ç¡®æ§åˆ¶ï¼Œè€Œæ— éœ€ä½¿ç”¨å¤æ‚çš„å¼ºåŒ–å­¦ä¹ ï¼Œä¹Ÿå¯ä»¥æœ‰æ•ˆå­¦ä¹ åˆ°äººç±»åå¥½ï¼ŒDPOç›¸è¾ƒäºRLHFæ›´å®¹æ˜“å®ç°ä¸”æ˜“äºè®­ç»ƒï¼Œæ•ˆæœæ›´å¥½
  - [GRPO](https://arxiv.org/pdf/2402.03300)é€šè¿‡ç»„å†…ç›¸å¯¹å¥–åŠ±æ¥ä¼°è®¡åŸºçº¿ï¼Œä»è€Œé¿å…ä½¿ç”¨é¢å¤–çš„ä»·å€¼å‡½æ•°æ¨¡å‹


## â–¶ï¸ Demo


æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€æ´çš„åŸºäºgradioçš„äº¤äº’å¼webç•Œé¢ï¼Œå¯åŠ¨æœåŠ¡åï¼Œå¯é€šè¿‡æµè§ˆå™¨è®¿é—®ï¼Œè¾“å…¥é—®é¢˜ï¼Œæ¨¡å‹ä¼šè¿”å›ç­”æ¡ˆã€‚

å¯åŠ¨æœåŠ¡ï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š
```shell
CUDA_VISIBLE_DEVICES=0 python gradio_demo.py --base_model path_to_llama_hf_dir --lora_model path_to_lora_dir
```

å‚æ•°è¯´æ˜ï¼š

- `--base_model {base_model}`ï¼šå­˜æ”¾HFæ ¼å¼çš„LLaMAæ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨HF Model Hubæ¨¡å‹è°ƒç”¨åç§°
- `--lora_model {lora_model}`ï¼šLoRAæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨HF Model Hubæ¨¡å‹è°ƒç”¨åç§°ã€‚è‹¥loraæƒé‡å·²ç»åˆå¹¶åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ™åˆ é™¤--lora_modelå‚æ•°
- `--tokenizer_path {tokenizer_path}`ï¼šå­˜æ”¾å¯¹åº”tokenizerçš„ç›®å½•ã€‚è‹¥ä¸æä¾›æ­¤å‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼ä¸--base_modelç›¸åŒ
- `--template_name`ï¼šæ¨¡æ¿åç§°ï¼Œå¦‚`vicuna`ã€`alpaca`ç­‰ã€‚è‹¥ä¸æä¾›æ­¤å‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼æ˜¯vicuna
- `--only_cpu`: ä»…ä½¿ç”¨CPUè¿›è¡Œæ¨ç†
- `--resize_emb`ï¼šæ˜¯å¦è°ƒæ•´embeddingå¤§å°ï¼Œè‹¥ä¸è°ƒæ•´ï¼Œåˆ™ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„embeddingå¤§å°ï¼Œé»˜è®¤ä¸è°ƒæ•´


## ğŸ’¾ Install
#### Updating the requirements

```markdown
git clone https://github.com/shibing624/MedicalGPT
cd MedicalGPT
pip install -r requirements.txt --upgrade
```

#### Hardware Requirement (æ˜¾å­˜/VRAM)


\* *ä¼°ç®—å€¼*

| è®­ç»ƒæ–¹æ³•  | ç²¾åº¦          |   7B  |  13B  |  30B  |   70B  |  110B  |  8x7B |  8x22B |
|-------|-------------| ----- | ----- | ----- | ------ | ------ | ----- | ------ |
| å…¨å‚æ•°   | AMP(è‡ªåŠ¨æ··åˆç²¾åº¦) | 120GB | 240GB | 600GB | 1200GB | 2000GB | 900GB | 2400GB |
| å…¨å‚æ•°   | 16          |  60GB | 120GB | 300GB |  600GB |  900GB | 400GB | 1200GB |
| LoRA  | 16          |  16GB |  32GB |  64GB |  160GB |  240GB | 120GB |  320GB |
| QLoRA | 8           |  10GB |  20GB |  40GB |   80GB |  140GB |  60GB |  160GB |
| QLoRA | 4           |   6GB |  12GB |  24GB |   48GB |   72GB |  30GB |   96GB |
| QLoRA | 2           |   4GB |   8GB |  16GB |   24GB |   48GB |  18GB |   48GB |

## ğŸš€ Training Pipeline

Training Stage:

| Stage                          | Introduction | Python script                                                                                           | Shell script                                                                  |
|:-------------------------------|:-------------|:--------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------|
| Continue Pretraining           | å¢é‡é¢„è®­ç»ƒ        | [pretraining.py]                     | [run_pt.sh]   |
| Supervised Fine-tuning         | æœ‰ç›‘ç£å¾®è°ƒ        | [supervised_finetuning.py]           | [run_sft.sh]   |
| Direct Preference Optimization | ç›´æ¥åå¥½ä¼˜åŒ–      | [dpo_training.py]                    | [run_dpo.sh]   |


- æä¾›åŸºäºçŸ¥è¯†åº“æ–‡ä»¶çš„LLMé—®ç­”åŠŸèƒ½ï¼ˆRAGï¼‰ï¼š[chatpdf.py]



## ğŸ’» Inference
è®­ç»ƒå®Œæˆåï¼Œç°åœ¨æˆ‘ä»¬åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ŒéªŒè¯æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„æ•ˆæœã€‚

```shell
CUDA_VISIBLE_DEVICES=0 python inference.py \
    --base_model path_to_model_hf_dir \
    --lora_model path_to_lora \
    --interactive
```

å‚æ•°è¯´æ˜ï¼š

- `--base_model {base_model}`ï¼šå­˜æ”¾HFæ ¼å¼çš„LLaMAæ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•
- `--tokenizer_path {base_model}`ï¼šå­˜æ”¾HFæ ¼å¼çš„LLaMAæ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•
- `--lora_model {lora_model}`ï¼šLoRAè§£å‹åæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨HF Model Hubæ¨¡å‹è°ƒç”¨åç§°ã€‚å¦‚æœå·²ç»åˆå¹¶äº†LoRAæƒé‡åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ™å¯ä»¥ä¸æä¾›æ­¤å‚æ•°
- `--tokenizer_path {tokenizer_path}`ï¼šå­˜æ”¾å¯¹åº”tokenizerçš„ç›®å½•ã€‚è‹¥ä¸æä¾›æ­¤å‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼ä¸--base_modelç›¸åŒ
- `--template_name`ï¼šæ¨¡æ¿åç§°ï¼Œå¦‚`vicuna`ã€`alpaca`ç­‰ã€‚è‹¥ä¸æä¾›æ­¤å‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼æ˜¯vicuna
- `--interactive`ï¼šä»¥äº¤äº’æ–¹å¼å¯åŠ¨å¤šè½®é—®ç­”ï¼Œä½¿ç”¨æµå¼æ¨ç†
- `--data_file {file_name}`ï¼šéäº¤äº’æ–¹å¼å¯åŠ¨ä¸‹ï¼Œè¯»å–file_nameä¸­çš„çš„å†…å®¹è¿›è¡Œbatché¢„æµ‹
- `--output_file {file_name}`ï¼šéäº¤äº’å¼æ–¹å¼ä¸‹ï¼Œå°†é¢„æµ‹çš„ç»“æœä»¥jsonlæ ¼å¼å†™å…¥file_name
- `--resize_emb`ï¼šæ˜¯å¦è°ƒæ•´embeddingå¤§å°ï¼Œè‹¥ä¸è°ƒæ•´ï¼Œåˆ™ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„embeddingå¤§å°ï¼Œé»˜è®¤ä¸è°ƒæ•´
- `--only_cpu`ï¼šä»…ä½¿ç”¨CPUè¿›è¡Œæ¨ç†
- `--gpus {gpu_ids}`ï¼šæŒ‡å®šä½¿ç”¨çš„GPUè®¾å¤‡ç¼–å·ï¼Œé»˜è®¤ä¸º0ã€‚å¦‚ä½¿ç”¨å¤šå¼ GPUï¼Œä»¥é€—å·åˆ†éš”ï¼Œå¦‚0,1,2

#### å¤šå¡æ¨ç†
å¤šå¡æ•°æ®å¹¶è¡Œï¼Œbatchæ¨ç†
```shell
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2 inference_multigpu_demo.py --base_model shibing624/vicuna-baichuan-13b-chat
```
#### vllmå¤šå¡éƒ¨ç½²
```shell
sh vllm_deployment.sh
```


## ğŸ“š Dataset
### åŒ»ç–—æ•°æ®é›†

- 240ä¸‡æ¡ä¸­æ–‡åŒ»ç–—æ•°æ®é›†(åŒ…æ‹¬é¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒå’Œå¥–åŠ±æ•°æ®é›†)ï¼š[shibing624/medical](https://huggingface.co/datasets/shibing624/medical)
- 22ä¸‡æ¡ä¸­æ–‡åŒ»ç–—å¯¹è¯æ•°æ®é›†(åä½—é¡¹ç›®)ï¼š[shibing624/huatuo_medical_qa_sharegpt](https://huggingface.co/datasets/shibing624/huatuo_medical_qa_sharegpt) ã€æœ¬é¡¹ç›®æ”¯æŒæ ¼å¼ã€‘

### é€šç”¨æ•°æ®é›†

#### Pretraining datasets(é¢„è®­ç»ƒæ•°æ®é›†)
- 16GBä¸­è‹±æ–‡æ— ç›‘ç£ã€å¹³è¡Œè¯­æ–™[Linly-AI/Chinese-pretraining-dataset](https://huggingface.co/datasets/Linly-AI/Chinese-pretraining-dataset)
- 524MBä¸­æ–‡ç»´åŸºç™¾ç§‘è¯­æ–™[wikipedia-cn-20230720-filtered](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered)
#### Supervised fine-tuning datasets(æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†)
- 10ä¸‡æ¡å¤šè¯­è¨€ShareGPT GPT4å¤šè½®å¯¹è¯æ•°æ®é›†ï¼š[shibing624/sharegpt_gpt4](https://huggingface.co/datasets/shibing624/sharegpt_gpt4) ã€æœ¬é¡¹ç›®æ”¯æŒæ ¼å¼ã€‘
- 9ä¸‡æ¡è‹±æ–‡ShareGPTå¤šè½®å¯¹è¯æ•°é›†ï¼š[anon8231489123/ShareGPT_Vicuna_unfiltered](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered) ã€æœ¬é¡¹ç›®æ”¯æŒæ ¼å¼ã€‘
- 50ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†ï¼š[BelleGroup/train_0.5M_CN](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)
- 100ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Belleæ•°æ®é›†ï¼š[BelleGroup/train_1M_CN](https://huggingface.co/datasets/BelleGroup/train_1M_CN)
- 5ä¸‡æ¡è‹±æ–‡ChatGPTæŒ‡ä»¤Alpacaæ•°æ®é›†ï¼š[50k English Stanford Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca#data-release)
- 2ä¸‡æ¡ä¸­æ–‡ChatGPTæŒ‡ä»¤Alpacaæ•°æ®é›†ï¼š[shibing624/alpaca-zh](https://huggingface.co/datasets/shibing624/alpaca-zh)
- 69ä¸‡æ¡ä¸­æ–‡æŒ‡ä»¤Guanacoæ•°æ®é›†(Belle50ä¸‡æ¡+Guanaco19ä¸‡æ¡)ï¼š[Chinese-Vicuna/guanaco_belle_merge_v1.0](https://huggingface.co/datasets/Chinese-Vicuna/guanaco_belle_merge_v1.0)
- 5ä¸‡æ¡è‹±æ–‡ChatGPTå¤šè½®å¯¹è¯æ•°æ®é›†ï¼š[RyokoAI/ShareGPT52K](https://huggingface.co/datasets/RyokoAI/ShareGPT52K)
- 80ä¸‡æ¡ä¸­æ–‡ChatGPTå¤šè½®å¯¹è¯æ•°æ®é›†ï¼š[BelleGroup/multiturn_chat_0.8M](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)
- 116ä¸‡æ¡ä¸­æ–‡ChatGPTå¤šè½®å¯¹è¯æ•°æ®é›†ï¼š[fnlp/moss-002-sft-data](https://huggingface.co/datasets/fnlp/moss-002-sft-data)
- 3.8ä¸‡æ¡ä¸­æ–‡ShareGPTå¤šè½®å¯¹è¯æ•°æ®é›†ï¼š[FreedomIntelligence/ShareGPT-CN](https://huggingface.co/datasets/FreedomIntelligence/ShareGPT-CN)
- 130ä¸‡æ¡ä¸­æ–‡å¾®è°ƒæ•°æ®é›†ï¼ˆæ±‡æ€»ï¼‰ï¼š[zhuangxialie/Llama3-Chinese-Dataset](https://modelscope.cn/datasets/zhuangxialie/Llama3-Chinese-Dataset/dataPeview) ã€æœ¬é¡¹ç›®æ”¯æŒæ ¼å¼ã€‘
- 7åƒæ¡ä¸­æ–‡è§’è‰²æ‰®æ¼”å¤šè½®å¯¹è¯æ•°æ®é›†ï¼š[shibing624/roleplay-zh-sharegpt-gpt4-data](https://huggingface.co/datasets/shibing624/roleplay-zh-sharegpt-gpt4-data) ã€æœ¬é¡¹ç›®æ”¯æŒæ ¼å¼ã€‘

#### Preference datasets(åå¥½æ•°æ®é›†)
- 2ä¸‡æ¡ä¸­è‹±æ–‡åå¥½æ•°æ®é›†ï¼š[shibing624/DPO-En-Zh-20k-Preference](https://huggingface.co/datasets/shibing624/DPO-En-Zh-20k-Preference) ã€æœ¬é¡¹ç›®æ”¯æŒæ ¼å¼ã€‘
- åŸç‰ˆçš„oasst1æ•°æ®é›†ï¼š[OpenAssistant/oasst1](https://huggingface.co/datasets/OpenAssistant/oasst1)
- 2ä¸‡æ¡å¤šè¯­è¨€oasst1çš„rewardæ•°æ®é›†ï¼š[tasksource/oasst1_pairwise_rlhf_reward](https://huggingface.co/datasets/tasksource/oasst1_pairwise_rlhf_reward)
- 11ä¸‡æ¡è‹±æ–‡hh-rlhfçš„rewardæ•°æ®é›†ï¼š[Dahoas/full-hh-rlhf](https://huggingface.co/datasets/Dahoas/full-hh-rlhf)
- 9ä¸‡æ¡è‹±æ–‡rewardæ•°æ®é›†(æ¥è‡ªAnthropic's Helpful Harmless dataset)ï¼š[Dahoas/static-hh](https://huggingface.co/datasets/Dahoas/static-hh)
- 7ä¸‡æ¡è‹±æ–‡rewardæ•°æ®é›†ï¼ˆæ¥æºåŒä¸Šï¼‰ï¼š[Dahoas/rm-static](https://huggingface.co/datasets/Dahoas/rm-static)
- 7ä¸‡æ¡ç¹ä½“ä¸­æ–‡çš„rewardæ•°æ®é›†ï¼ˆç¿»è¯‘è‡ªrm-staticï¼‰[liswei/rm-static-m2m100-zh](https://huggingface.co/datasets/liswei/rm-static-m2m100-zh)
- 7ä¸‡æ¡è‹±æ–‡Rewardæ•°æ®é›†ï¼š[yitingxie/rlhf-reward-datasets](https://huggingface.co/datasets/yitingxie/rlhf-reward-datasets)
- 3åƒæ¡ä¸­æ–‡çŸ¥ä¹é—®ç­”åå¥½æ•°æ®é›†ï¼š[liyucheng/zhihu_rlhf_3k](https://huggingface.co/datasets/liyucheng/zhihu_rlhf_3k)




## ğŸ’• Acknowledgements

- [MedicalGPT: Training Medical GPT Model]((https://github.com/shibing624/MedicalGPT))
- [Direct Preference Optimization:Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290.pdf)
- [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora/blob/main/finetune.py)
- [ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)
- [hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)
- [dvlab-research/LongLoRA](https://github.com/dvlab-research/LongLoRA)

Thanks for their great work!


